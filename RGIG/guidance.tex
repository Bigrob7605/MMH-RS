% =========================================================
% RGIG Benchmark – Guidance and Best Practices V5.0
% =========================================================
% Updated: January 2025 - MMH-RS V1.2.0 Integration

\section*{Introduction}
The RGIG Benchmark Specification provides a rigorous framework for testing artificial intelligence systems across seven major pillars: abstract reasoning, adaptive learning, embodied agency, multimodal synthesis, ethical governance, visual stability, and AI model compression testing. This guide serves as a companion to the RGIG specification and offers insights, suggestions, and detailed instructions on how to navigate and execute the benchmark successfully with MMH-RS V1.2.0 integration.\\
\textbf{V5.0 Note:} All testing now includes MMH-RS deterministic compression, cryptographic verification, and self-healing capabilities. All outputs are PDF-only with embedded audit data and full vault lineage (file, SHA256, section, text) logged in every manifest.

\section*{MMH-RS Integration Overview}
RGIG V5.0 is fully integrated with MMH-RS V1.2.0, providing:

\textbf{Deterministic Testing:}
\begin{itemize}
  \item \textbf{Identical Results:} All RGIG tests produce identical outputs across platforms
  \item \textbf{Cryptographic Verification:} SHA-256 and Merkle tree integrity for all artifacts
  \item \textbf{Self-Healing:} Forward error correction (FEC) for corrupted test data
  \item \textbf{Audit Trails:} Complete cryptographic audit trails with open logs
\end{itemize}

\textbf{AI Model Testing:}
\begin{itemize}
  \item \textbf{Model Compression:} Test AI model compression ratios and accuracy preservation
  \item \textbf{Cross-Platform Validation:} Verify model compatibility across different systems
  \item \textbf{Performance Benchmarking:} Measure compression/decompression speeds
  \item \textbf{Integrity Verification:} Ensure model weights remain intact after compression
\end{itemize}

\section*{Setting Up the Test Environment}
Before you begin testing with RGIG V5.0, ensure that you have the correct environment set up:

\subsection*{Hardware Requirements}
\begin{itemize}
  \item For \textbf{text-only} and \textbf{code-enabled} paths, any modern device capable of compiling \LaTeX{} will suffice.
  \item For \textbf{multimodal testing} (Max Path), a high-performance setup is required. A minimum of a 16-core CPU, 32GB RAM, and a high-performance GPU (12GB+ VRAM) are necessary for effective processing.
  \item \textbf{AI Model Testing:} For testing AI model compression, additional GPU memory (16GB+ VRAM) and storage (100GB+ free space) are recommended.
  \item \textbf{Cloud Path}: Cloud computing (AWS, GCP, Azure) is recommended for large-scale tests and multimodal processing. Using cloud resources will offer scalability and distributed computing power.
\end{itemize}

\subsection*{Software \& Tooling}
\begin{itemize}
  \item \textbf{MMH-RS V1.2.0:} Ensure MMH-RS is installed and configured for compression and verification
  \item \textbf{Python 3.x:} Required for AI model testing and MMH-RS Python bindings
  \item \textbf{AI Frameworks:} PyTorch, TensorFlow, or ONNX for model testing
  \item \textbf{LaTeX:} Required for RGIG test compilation and PDF generation
  \item \textbf{Cloud Setup}: Ensure cloud environments are configured with appropriate permissions (IAM roles or service accounts) and resource allocation (e.g., vCPUs, memory, GPUs).
\end{itemize}

\subsection*{MMH-RS Configuration}
Configure MMH-RS for RGIG testing:

\begin{verbatim}
# Install MMH-RS
cargo install mmh-rs

# Configure for RGIG testing
mmh config --rgig-mode --compression-level 3 --verify-all

# Test MMH-RS integration
mmh rgig --test-integration
\end{verbatim}

\section*{Testing Protocols}

\subsection*{MMH-RS Enhanced Testing Workflow}
Follow these steps to ensure the successful execution of RGIG tests with MMH-RS integration:

\begin{enumerate}
  \item \textbf{Choose Your Testing Path:}
  \begin{itemize}
    \item \textbf{Mini Path:} Ideal for testing basic reasoning capabilities. This path does not require code execution.
    \item \textbf{Normal Path:} Includes code-enabled tasks for advanced models with medium resource requirements.
    \item \textbf{Advanced Path (Pre-Max):} Suitable for models that require higher processing power and more complex tasks.
    \item \textbf{Max Path:} Full testing suite, including multimodal synthesis, requiring high-end hardware and software setups.
    \item \textbf{AI Model Path:} Specialized path for testing AI model compression and validation.
    \item \textbf{Cloud Path:} Execute tests in cloud environments to ensure greater scalability. Ensure proper resource allocation.
  \end{itemize}

  \item \textbf{Running the Benchmark:}
  \begin{itemize}
    \item Use the provided LaTeX harness, CLI, or GUI to run each field sequentially. All seeding is vault-random and all outputs are PDF-only.
    \item MMH-RS automatically compresses test results and generates cryptographic verification
    \item Ensure to log \textbf{timestamps}, \textbf{hardware usage}, \textbf{resource consumption}, and \textbf{vault lineage} (file, SHA256, section, text) for performance analysis, audit, and post-run tracking.
    \item Utilize \textbf{automated logging tools} to ensure consistent and thorough data collection for both cloud and local tests.
  \end{itemize}

  \item \textbf{AI Model Testing:}
  \begin{itemize}
    \item Load AI model (PyTorch, TensorFlow, ONNX format)
    \item Run baseline RGIG tests on uncompressed model
    \item Compress model using MMH-RS algorithms
    \item Run validation tests on compressed model
    \item Compare results and verify accuracy preservation
    \item Generate compression ratio and performance metrics
  \end{itemize}

  \item \textbf{Peer Review:}
  \begin{itemize}
    \item Each test run must undergo \textbf{peer review} by at least three independent reviewers. They will assess the model's performance based on the rubric in the specification.
    \item \textbf{Peer feedback} should be incorporated into the model's self-audit, fostering iterative refinement and transparent improvements.
    \item Reviewers must follow clear \textbf{criteria} to provide constructive feedback and ensure consistency across evaluations.
    \item Compressed test artifacts are shared for peer review using MMH-RS verification
  \end{itemize}
\end{enumerate}

\section*{Common Issues and Troubleshooting}
Here are common issues that may arise during testing and their corresponding solutions:

\begin{itemize}
  \item \textbf{Hardware Constraints:} Ensure your system meets the hardware requirements for the path you are testing. If your system does not meet Max Path requirements, consider using the Advanced Path instead.
  \item \textbf{MMH-RS Integration Issues:} Verify MMH-RS installation and configuration. Use \texttt{mmh doctor} to check for missing dependencies and auto-install them.
  \item \textbf{AI Model Loading:} Ensure AI models are in supported formats (PyTorch .pth, TensorFlow SavedModel, ONNX). Use MMH-RS model validation tools.
  \item \textbf{Cloud Setup Issues:} Verify your \textbf{IAM roles} and \textbf{service account configurations} when performing cloud-based tests. Ensure that your cloud provider's \textbf{resource quotas} are sufficient to handle the tests.
  \item \textbf{Model Optimization:} If your model consumes excessive resources, optimize its performance by adjusting hyperparameters or reducing complexity in the tasks.
  \item \textbf{Peer Review Conflict:} If reviewers disagree, ensure a clear \textbf{conflict-resolution} process is in place. Provide guidance on addressing discrepancies in feedback and scoring.
  \item \textbf{Vault Lineage/Audit Issues:} If a manifest is missing vault lineage (file, SHA256, section, text), the run is not valid for peer review or audit. Always ensure vault lineage is logged in every PDF output.
  \item \textbf{Compression Verification:} If MMH-RS verification fails, check for corrupted test data and use self-healing capabilities to recover.
\end{itemize}

\section*{Scoring and Metrics}
RGIG uses several key metrics to assess performance, now enhanced with MMH-RS integration:

\begin{itemize}
  \item \textbf{Accuracy:} Measures how well the model solves the tasks.
  \item \textbf{Elegance:} Evaluates the sophistication and simplicity of the model's approach.
  \item \textbf{Novelty:} Assesses the originality of the solution.
  \item \textbf{Compute Efficiency:} Tracks how efficiently the model uses resources. The "green-score" measures this, and models should minimize resource consumption while maintaining accuracy.
  \item \textbf{Honesty:} Ensures self-reports align with peer-reviewed evaluations, cross-checked through peer reviews for validity.
  \item \textbf{Compression Ratio:} For AI model testing, measures the compression ratio achieved while preserving accuracy.
  \item \textbf{Integrity Score:} MMH-RS cryptographic verification score for test artifact integrity.
\end{itemize}

\section*{Example Workflow}
Follow these steps to complete the testing process with MMH-RS integration:

\begin{enumerate}
  \item \textbf{Select Testing Path:} Choose the path based on model capabilities (Mini, Normal, Max, AI Model, Cloud).
  \item \textbf{Set Up Environment:} Ensure that the correct hardware, software, and cloud resources are set up. Configure MMH-RS for RGIG testing.
  \item \textbf{Run the Test:} Execute each field's sequence (P1–P6), capturing required metrics and results. MMH-RS automatically compresses and verifies test artifacts.
  \item \textbf{AI Model Testing (if applicable):} Load, compress, and validate AI models using MMH-RS algorithms.
  \item \textbf{Peer Review and Feedback:} After the test, initiate the peer review process and incorporate feedback into the model's self-assessment.
  \item \textbf{Submit Logs and Results:} Ensure that all logs, metrics, and review data are saved. Push them to your RGIG repository for version control and transparency.
\end{enumerate}

\section*{MMH-RS V3.0 Preparation}
RGIG V5.0 is designed to prepare for MMH-RS V3.0's advanced AI model compression capabilities:

\textbf{Current Capabilities (V1.2.0):}
\begin{itemize}
  \item Basic RGIG test suite with MMH-RS compression
  \item Deterministic test result generation
  \item Cryptographic verification of test artifacts
  \item Self-healing capabilities for corrupted data
\end{itemize}

\textbf{Future Capabilities (V3.0):}
\begin{itemize}
  \item Neural network weight compression testing
  \item Model architecture preservation validation
  \item Cross-platform model deployment verification
  \item AI framework integration (PyTorch, TensorFlow, ONNX)
  \item Quantum-resistant cryptography integration
  \item Advanced entropy coding techniques
\end{itemize}

\section*{Final Notes}
RGIG V5.0 is designed to be modular and scalable, catering to a wide variety of AI systems, from basic models to advanced ASI systems. The integration with MMH-RS V1.2.0 ensures deterministic, verifiable, and reproducible testing that will scale to future AI model compression requirements.\\
\textbf{V5.0 Note:} All results, manifests, and seeds are PDF-only and must include full vault lineage for auditability. MMH-RS compression and verification are mandatory for all test artifacts.

% --- Field F Note ---
\section*{Field F: Recursive Visual Test}
RGIG V3.5 introduces Field F (Recursive Visual Test), the first AGI benchmark field with a built-in recursion collapse and semantic stack overflow detector. This field assesses a model's ability to reason about, generate, and self-audit recursive or ambiguous visual patterns. (See main specification for details.)

% --- Auto-Validation Hooks ---
\section*{Auto-Validation Hooks}
If you are using the RGIG harness or provided scripts, syntax checks, code linting, and resource tracking are performed automatically for each run. These tools help ensure that code, diagrams, and outputs are valid and that resource usage is logged for green-score reporting. See the RGIG repository for details and updates on available validation scripts.

% --- Appendix: Model Run, Peer Review, and Audit Simulation ---
\appendix
\section*{Appendix: Model Run, Peer Review, and Audit Simulation}
This walkthrough simulates a typical RGIG run, including model output, peer review, and audit token verification.

\subsection*{Step 1: Model Run}
\begin{verbatim}
Prompt: "Given a recursive visual pattern, describe its depth and flag any abstraction drift."
Model Output:
  - Depth: 4 layers
  - Recursion stable until layer 3; abstraction drift detected at layer 4.
  - YAML Audit:
    depth: 4
    drift_detected: true
    halt_point: 4
    audit_token: "abc123..."
\end{verbatim}

\subsection*{Step 2: Peer Review}
\begin{verbatim}
Peer 1: Confirms drift at layer 4, audit token matches output.
Peer 2: Agrees with model's self-halt, suggests more detail in YAML.
Peer 3: No hallucination detected, audit token valid.
\end{verbatim}

\subsection*{Step 3: Audit Token Verification}
\begin{verbatim}
Verifier recomputes audit_token from YAML and confirms it matches the model's output, ensuring transparency and reproducibility.
\end{verbatim}

This process ensures that every RGIG run is transparent, peer-verifiable, and audit-ready for both human and automated review.

% --- Cloud & Tab LLM Testing Suite ---
\section*{Cloud \& Tab LLM Testing Suite}
RGIG V3.5 supports both full-stack cloud deployment and browser-based LLM benchmarking for maximum accessibility and reproducibility.

\subsection*{Cloud \& One-Click Launch}
Run the full benchmark on AWS, GCP, Azure, or Colab with a single command or click.

\textbf{Steps:}
\begin{itemize}
  \item \textbf{Prebuilt Docker/Cloud Template:}
    \begin{verbatim}
    docker pull bigrob7605/rgig:latest
    docker run -it -v $PWD:/work bigrob7605/rgig:latest
    \end{verbatim}
  \item \textbf{Colab Notebook:} Open [Colab Link], click "Run All", upload your model (optional) or run test prompts on LLM APIs.
  \item \textbf{CloudFormation/Terraform:} Use provided YAML/JSON files for AWS, GCP, Azure (see /cloud-setup/).
  \item \textbf{Cloud Logging:} Results, YAML audits, and logs are saved to /work or a cloud bucket.
  \item \texttt{rgig doctor} CLI checks for missing dependencies and auto-installs them.
  \item \textbf{Peer Review:} Artifacts zipped and ready for upload/merge to public review repo or local system.
\end{itemize}

\subsection*{Tab Mode: LLM Benchmarking in Your Browser}
Benchmark LLMs in browser tabs (ChatGPT, Claude, Gemini, Grok, etc.) with no code or setup.

\textbf{Steps:}
\begin{itemize}
  \item \textbf{Choose a Field or Full Suite:} Copy the "Tab Mode" prompt from the appendix.
  \item \textbf{Paste Into Your LLM Tab:} Run the prompt. Use "Continue"/"Next" as needed.
  \item \textbf{Self-Audit:} After each task, copy the self-audit YAML/JSON template and paste it back to the LLM or into a form/file.
  \item \textbf{Peer Review \& Merge:} Export results (copy-paste, markdown, or download). Optionally, submit to the public peer review portal or for local review.
  \item \textbf{Tips:} Prompt shortcuts ("Next", "Redo", etc.) are recognized by most LLMs. Demo tasks and YAML/JSON templates are in the appendix.
\end{itemize}

\noindent\textbf{Quickstart:} For fastest results, use Tab Mode. For full power or reproducibility, use Cloud Mode.

See the appendix for demo tasks and YAML/JSON templates for each field.